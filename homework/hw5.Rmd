---
title: "Homework #5"
author: "**Your Name Here**"
date: "Due: Tue Oct 8 2:00pm"
output: html_document
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=FALSE,     # do not show R warnings
                      message=FALSE)     # do not show R messages
```

<!--- Solution Region --->
<style>
#solution {
  background-color: #8FBC8F;
  border-style: solid;
  border-color: blue;
  padding: .5em;
  margin: 20px
}
</style>



**SYS 6018 | Fall 2019 | University of Virginia **

*******************************************

<!--- Load Required R packages here --->
```{r, include=FALSE}
library(tidyverse)
digits <- function(x, k=2) format(round(x, k), nsmall=k)
data.dir = 'https://raw.githubusercontent.com/mdporter/SYS6018/master/data'

#-- Useful R Code. See anomaly.R and class notes for more examples
# the table() function calculates the frequency of the values in a vector
# the dplyr functions count() calculates the frequency of the values in a data frame
# colSums() computes column sums of a matrix or data frame
```



### Problem 5.1: UVA Medical Center Payments

The website [OpenPaymentsData.CMS.gov](OpenPaymentsData.CMS.gov) provides data on the payments made by drug and medical device companies to physicians and teaching hospitals. I have extracted the payments made to the University of Virginia Medical Center for research activities. This exercise will evaluate if the results conform to Benford's Law. 

The data can be found at <https://raw.githubusercontent.com/mdporter/SYS6018/master/data/research_payments.csv>

a. Load the data, extract the first digit from the `payment` column, and calculate the counts of each digit, $1,2,\ldots, 9$. Show a table or data frame of the counts. 

<div id="solution">
SOLUTION HERE
</div>


b. Calculate the $\chi^2$ and *log likelihood ratio* test statistics. Use Benford's distribution as the null model. 
    - Use the MLE for the alternative distribution for the log-likelihood ratio test statistic

<div id="solution">
SOLUTION HERE
</div>

c. Estimate the $p$-value for the two test statistics. Indicate if you used simulation or theoretical/asymptotic distributions. Do the results support the null hypothesis that the data come from Benford's distribution?  

<div id="solution">
SOLUTION HERE
</div>


d. This exercise will help you measure the uncertainty in the counts from Benford's distribution. Simulate $M=10000$ realizations from Benford's distribution and compute the approximate 95\% pointwise prediction intervals for the *frequency* of each digit.
    - Each simulation will generate $n$ first digits, where $n$ is the number of observations in the Medical Payments data from part a. 
    - Estimate the 95% prediction intervals for each digit. These are the 2.5% and 97.5% quantiles. 
    - Show a table of the prediction intervals. It should include the digit {1,2, \ldots, 9}, the lower bound, and the upper bound. Again, approximately 95% of the simulated data should be inside the intervals. 
    - Produce a visual display of the 95% intervals. Include the observed data. 
    - What does this analysis suggest about the nature of conformity or nonconformity to Benford's distribution? 

<div id="solution">
SOLUTION HERE
</div>



### Problem 5.2: Interstate Crash Hotspots

In HW 3.2 (Interstate Crash Density), part e, you were asked to identify the *most dangerous* (mile, time) pair. Here we will attempt this with hotspot analysis. 

As we discussed in class, a hotspot model can be formulated in the form of a mixture model. Specifically, if a uniform distribution is specified for the null model and the hotspot components are Gaussian then the pdf becomes
$$f(x; \theta) = \frac{\pi_0}{V} + \sum_{k=1}^K \pi_i \mathcal{N}(\mathbf{x}| \mathbf{\mu}_k, \Sigma_k)$$

- $V$ is the volume of the [minimum bounding box](https://en.wikipedia.org/wiki/Minimum_bounding_box)
- $\pi_0 + \sum_{k=1}^K \pi_k = 1$
- $K$ is the number of *hotspots*

a. Fit this model. You can use your own code or something existing, e.g. `Mclust(..., initialization=list(noise=TRUE))` from the `mclust` R package. Explain any settings or tuning parameters you used. Show your code. 


<div id="solution">
SOLUTION HERE
</div>


b. Report what you found for $K$, the number of hotspots. 

<div id="solution">
SOLUTION HERE
</div>

c. Report the centroid(s) of the hotspots. 

<div id="solution">
SOLUTION HERE
</div>

d. What are the estimated number of observations in each hotspot?

<div id="solution">
SOLUTION HERE
</div>

e. Produce a scatterplot of the observations, indicating by color or shape the observations that are most likely in the hotspot. 

<div id="solution">
SOLUTION HERE
</div>



